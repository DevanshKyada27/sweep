{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates on the Chunking Algorithm\n",
    "This notebook is for the blog on improvements to our chunking algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "\n",
    "language = get_language('python')\n",
    "parser = get_parser('python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Span:\n",
    "    # Represents a slice of a string\n",
    "    start: int = 0\n",
    "    end: int = 0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # If end is None, set it to start\n",
    "        if self.end is None:\n",
    "            self.end = self.start\n",
    "\n",
    "    def extract(self, s: str) -> str:\n",
    "        # Grab the corresponding substring of string s by bytes\n",
    "        return s[self.start: self.end]\n",
    "\n",
    "    def extract_lines(self, s: str) -> str:\n",
    "        # Grab the corresponding substring of string s by lines\n",
    "        return \"\\n\".join(s.splitlines()[self.start:self.end])\n",
    "\n",
    "    def __add__(self, other: Span | int) -> Span:\n",
    "        # e.g. Span(1, 2) + Span(2, 4) = Span(1, 4) (concatenation)\n",
    "        # There are no safety checks: Span(a, b) + Span(c, d) = Span(a, d)\n",
    "        # and there are no requirements for b = c.\n",
    "        if isinstance(other, int):\n",
    "            return Span(self.start + other, self.end + other)\n",
    "        elif isinstance(other, Span):\n",
    "            return Span(self.start, other.end)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # i.e. Span(a, b) = b - a\n",
    "        return self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example code we're gonna use in this guide will be from https://github.com/sweepai/sweep/blob/b267b613d4c706eaf959fe6789f11e9a856521d1/sweepai/handlers/on_check_suite.py, our old handler for parsing GitHub Action run logs at Sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "example_file = \"https://raw.githubusercontent.com/sweepai/sweep/b267b613d4c706eaf959fe6789f11e9a856521d1/sweepai/handlers/on_check_suite.py\"\n",
    "python_code = requests.get(example_file).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the syntax tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression_statement:431-695\n",
      "  assignment:431-695\n",
      "    string:445-695\n",
      "function_definition:697-1501\n",
      "  block:776-1501\n",
      "    expression_statement:776-952\n",
      "      assignment:776-952\n",
      "        dictionary:786-952\n",
      "    expression_statement:957-1102\n",
      "      assignment:957-1102\n",
      "        call:968-1102\n",
      "    if_statement:1126-1481\n",
      "      block:1166-1399\n",
      "        for_statement:1231-1399\n",
      "function_definition:1504-2106\n",
      "  block:1539-2106\n",
      "    expression_statement:1642-1993\n",
      "      assignment:1642-1993\n",
      "        list:1653-1993\n",
      "function_definition:2109-4425\n",
      "  block:2161-4425\n",
      "    if_statement:2354-2511\n",
      "      block:2392-2511\n",
      "    if_statement:3211-3396\n",
      "      block:3257-3396\n",
      "        expression_statement:3257-3396\n",
      "          augmented_assignment:3257-3396\n",
      "    if_statement:3446-3858\n",
      "      block:3562-3858\n",
      "        expression_statement:3562-3745\n",
      "          assignment:3562-3745\n",
      "            call:3572-3745\n",
      "              argument_list:3600-3745\n",
      "                binary_operator:3601-3744\n",
      "    expression_statement:3983-4396\n",
      "      call:3983-4396\n",
      "        argument_list:3993-4396\n"
     ]
    }
   ],
   "source": [
    "tree = parser.parse(python_code.encode(\"utf-8\"))\n",
    "\n",
    "def pretty_node(node):\n",
    "    return f\"{node.type}:{node.start_byte}-{node.end_byte}\"\n",
    "\n",
    "def print_tree(node, indent=\"\"):\n",
    "    if len(re.sub(\"\\s\", \"\", node.text.decode(\"utf-8\"))) < 100:\n",
    "        return\n",
    "    print(indent + pretty_node(node))\n",
    "    for child in node.children:\n",
    "        print_tree(child, indent=indent + \"  \")\n",
    "\n",
    "for child in tree.root_node.children:\n",
    "    print_tree(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it doesn't actually line up:\n",
    "\n",
    "```python\n",
    "expression_statement:432-696\n",
    "  assignment:432-696\n",
    "    string:446-696\n",
    "function_definition:698-1502\n",
    "  block:777-1502\n",
    "    expression_statement:777-953\n",
    "      assignment:777-953\n",
    "        dictionary:787-953\n",
    "```\n",
    "\n",
    "Notice that the “expression_statement” ends on byte 696 and “function_definition” starts on byte 698, skipping a byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chunks(chunks: list[Span]):\n",
    "    for prev, curr in zip(chunks[:-1], chunks[1:]):\n",
    "        prev.end = curr.start\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coalescing\n",
    "\n",
    "Here was the algo presented in the last blog. Unfortunately it has some bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ioimport osimport zipfileimport openaiimport requestsfrom loguru import loggerfrom sweepai.core.gha_extraction import GHAExtractorfrom sweepai.events import CheckRunCompletedfrom sweepai.handlers.on_comment import on_commentfrom sweepai.utils.config.client import SweepConfig, get_gha_enabledfrom sweepai.utils.github_utils import get_github_client, get_tokenopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "log_message = \"\"\"GitHub actions yielded the following error. \n",
      "\n",
      "{error_logs}\n",
      "\n",
      "This is likely a linting or type-checking issue with the source code but if you are updating the GitHub Actions or versioning, this could be an issue with the GitHub Action yaml files.\"\"\"\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "defdownload_logs(repo_full_name: str, run_id: int, installation_id: int):\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "headers = {\n",
      "        \"Accept\": \"application/vnd.github+json\",\n",
      "        \"Authorization\": f\"Bearer {get_token(installation_id)}\",\n",
      "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
      "    }response = requests.get(f\"https://api.github.com/repos/{repo_full_name}/actions/runs/{run_id}/logs\",\n",
      "                            headers=headers)logs_str = \"\"\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "if response.status_code == 200:\n",
      "        zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
      "        for file in zip_file.namelist():\n",
      "            if \"/\" not in file:\n",
      "                with zip_file.open(file) as f:\n",
      "                    logs_str += f.read().decode(\"utf-8\")\n",
      "    else:\n",
      "        logger.warning(f\"Failed to download logs for run id: {run_id}\")return logs_str\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "defclean_logs(logs_str: str):log_list = logs_str.split(\"\\n\")\n",
      "    truncated_logs = [log[log.find(\" \") + 1:] for log in log_list]\n",
      "    patterns = [\n",
      "        # for docker\n",
      "        \"Already exists\",\n",
      "        \"Pulling fs layer\",\n",
      "        \"Waiting\",\n",
      "        \"Download complete\",\n",
      "        \"Verifying Checksum\",\n",
      "        \"Pull complete\",\n",
      "        # For github\n",
      "        \"remote: Counting objects\",\n",
      "        \"remote: Compressing objects:\",\n",
      "        \"Receiving objects:\",\n",
      "        \"Resolving deltas:\"\n",
      "    ]\n",
      "    return \"\\n\".join([log.strip() for log in truncated_logs if not any(pattern in log for pattern in patterns)])\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "defon_check_suite(request: CheckRunCompleted):\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "logger.info(f\"Received check run completed event for {request.repository.full_name}\")g = get_github_client(request.installation.id)repo = g.get_repo(request.repository.full_name)if not get_gha_enabled(repo):\n",
      "        logger.info(f\"Skipping github action for {request.repository.full_name} because it is not enabled\")\n",
      "        return Nonepr = repo.get_pull(request.check_run.pull_requests[0].number)num_pr_commits = len(list(pr.get_commits()))if num_pr_commits > 20:\n",
      "        logger.info(f\"Skipping github action for PR with {num_pr_commits} commits\")\n",
      "        return None\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "logger.info(f\"Running github action for PR with {num_pr_commits} commits\")logs = download_logs(\n",
      "        request.repository.full_name,\n",
      "        request.check_run.run_id,\n",
      "        request.installation.id\n",
      "    )if not logs:\n",
      "        return Nonelogs = clean_logs(logs)extractor = GHAExtractor()logger.info(f\"Extracting logs from {request.repository.full_name}, logs: {logs}\")problematic_logs = extractor.gha_extract(logs)if problematic_logs.count(\"\\n\") > 15:\n",
      "        problematic_logs += \"\\n\\nThere are a lot of errors. This is likely a larger issue with the PR and not a small linting/type-checking issue.\"\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "comments = list(pr.get_issue_comments())if len(comments) >= 2 and problematic_logs == comments[-1].body and comments[-2].body == comments[-1].body:\n",
      "        comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs) + \"\\n\\nI'm getting the same errors 3 times in a row, so I will stop working on fixing this PR.\")\n",
      "        logger.warning(\"Skipping logs because it is duplicated\")\n",
      "        raise Exception(\"Duplicate error logs\")print(problematic_logs)comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs))\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "on_comment(\n",
      "        repo_full_name=request.repository.full_name,\n",
      "        repo_description=request.repository.description,\n",
      "        comment=problematic_logs,\n",
      "        pr_path=None,\n",
      "        pr_line_position=None,\n",
      "        username=request.sender.login,\n",
      "        installation_id=request.installation.id,\n",
      "        pr_number=request.check_run.pull_requests[0].number,\n",
      "        comment_id=comment.id,\n",
      "        repo=repo,\n",
      "    )return {\"success\": True}\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tree_sitter import Node\n",
    "from dataclasses import field\n",
    "\n",
    "def chunk_node(\n",
    "    node: Node, \n",
    "    text: str, \n",
    "    MAX_CHARS: int = 600\n",
    ") -> list[str]:\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for child in node.children:\n",
    "        if child.end_byte - child.start_byte > MAX_CHARS:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = \"\"\n",
    "            chunks.extend(chunk_node(child, text, MAX_CHARS))\n",
    "        elif child.end_byte - child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = text[child.start_byte: child.end_byte]\n",
    "        else:\n",
    "            current_chunk += text[child.start_byte: child.end_byte]\n",
    "    chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "for chunk in chunk_node(tree.root_node, python_code):\n",
    "    print(chunk + \"\\n\\n====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is with the fixes by using the start_byte of the next node instead of the end_byte of the current node. \n",
    "\n",
    "I added a fake node at the end with start and end bytes equal to the end byte of the entire node. This is so that we don't need to rewrite the loop logic one last time for the last node. The purpose of MockNode is because the tree_sitter Node library doesn't have a constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import io\n",
      "import os\n",
      "import zipfile\n",
      "\n",
      "import openai\n",
      "import requests\n",
      "from loguru import logger\n",
      "\n",
      "from sweepai.core.gha_extraction import GHAExtractor\n",
      "from sweepai.events import CheckRunCompleted\n",
      "from sweepai.handlers.on_comment import on_comment\n",
      "from sweepai.utils.config.client import SweepConfig, get_gha_enabled\n",
      "from sweepai.utils.github_utils import get_github_client, get_token\n",
      "\n",
      "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "log_message = \"\"\"GitHub actions yielded the following error. \n",
      "\n",
      "{error_logs}\n",
      "\n",
      "This is likely a linting or type-checking issue with the source code but if you are updating the GitHub Actions or versioning, this could be an issue with the GitHub Action yaml files.\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "def download_logs(repo_full_name: str, run_id: int, installation_id: int):\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "headers = {\n",
      "        \"Accept\": \"application/vnd.github+json\",\n",
      "        \"Authorization\": f\"Bearer {get_token(installation_id)}\",\n",
      "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
      "    }\n",
      "    response = requests.get(f\"https://api.github.com/repos/{repo_full_name}/actions/runs/{run_id}/logs\",\n",
      "                            headers=headers)\n",
      "\n",
      "    logs_str = \"\"\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "if response.status_code == 200:\n",
      "        zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
      "        for file in zip_file.namelist():\n",
      "            if \"/\" not in file:\n",
      "                with zip_file.open(file) as f:\n",
      "                    logs_str += f.read().decode(\"utf-8\")\n",
      "    else:\n",
      "        logger.warning(f\"Failed to download logs for run id: {run_id}\")\n",
      "    return logs_str\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "def clean_logs(logs_str: str):\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "log_list = logs_str.split(\"\\n\")\n",
      "    truncated_logs = [log[log.find(\" \") + 1:] for log in log_list]\n",
      "    patterns = [\n",
      "        # for docker\n",
      "        \"Already exists\",\n",
      "        \"Pulling fs layer\",\n",
      "        \"Waiting\",\n",
      "        \"Download complete\",\n",
      "        \"Verifying Checksum\",\n",
      "        \"Pull complete\",\n",
      "        # For github\n",
      "        \"remote: Counting objects\",\n",
      "        \"remote: Compressing objects:\",\n",
      "        \"Receiving objects:\",\n",
      "        \"Resolving deltas:\"\n",
      "    ]\n",
      "    return \"\\n\".join([log.strip() for log in truncated_logs if not any(pattern in log for pattern in patterns)])\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "def on_check_suite(request: CheckRunCompleted):\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "logger.info(f\"Received check run completed event for {request.repository.full_name}\")\n",
      "    g = get_github_client(request.installation.id)\n",
      "    repo = g.get_repo(request.repository.full_name)\n",
      "    if not get_gha_enabled(repo):\n",
      "        logger.info(f\"Skipping github action for {request.repository.full_name} because it is not enabled\")\n",
      "        return None\n",
      "    pr = repo.get_pull(request.check_run.pull_requests[0].number)\n",
      "    num_pr_commits = len(list(pr.get_commits()))\n",
      "    if num_pr_commits > 20:\n",
      "        logger.info(f\"Skipping github action for PR with {num_pr_commits} commits\")\n",
      "        return None\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "logger.info(f\"Running github action for PR with {num_pr_commits} commits\")\n",
      "    logs = download_logs(\n",
      "        request.repository.full_name,\n",
      "        request.check_run.run_id,\n",
      "        request.installation.id\n",
      "    )\n",
      "    if not logs:\n",
      "        return None\n",
      "    logs = clean_logs(logs)\n",
      "    extractor = GHAExtractor()\n",
      "    logger.info(f\"Extracting logs from {request.repository.full_name}, logs: {logs}\")\n",
      "    problematic_logs = extractor.gha_extract(logs)\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "if problematic_logs.count(\"\\n\") > 15:\n",
      "        problematic_logs += \"\\n\\nThere are a lot of errors. This is likely a larger issue with the PR and not a small linting/type-checking issue.\"\n",
      "    comments = list(pr.get_issue_comments())\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "if len(comments) >= 2 and problematic_logs == comments[-1].body and comments[-2].body == comments[-1].body:\n",
      "        comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs) + \"\\n\\nI'm getting the same errors 3 times in a row, so I will stop working on fixing this PR.\")\n",
      "        logger.warning(\"Skipping logs because it is duplicated\")\n",
      "        raise Exception(\"Duplicate error logs\")\n",
      "    print(problematic_logs)\n",
      "    comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs))\n",
      "    \n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "on_comment(\n",
      "        repo_full_name=request.repository.full_name,\n",
      "        repo_description=request.repository.description,\n",
      "        comment=problematic_logs,\n",
      "        pr_path=None,\n",
      "        pr_line_position=None,\n",
      "        username=request.sender.login,\n",
      "        installation_id=request.installation.id,\n",
      "        pr_number=request.check_run.pull_requests[0].number,\n",
      "        comment_id=comment.id,\n",
      "        repo=repo,\n",
      "    )\n",
      "    return {\"success\": True}\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class MockNode:\n",
    "    start_byte: int = 0\n",
    "    end_byte: int = 0\n",
    "    children: list[MockNode] = field(default_factory=list)\n",
    "\n",
    "def chunk_node(\n",
    "    node: Node, \n",
    "    text: str, \n",
    "    MAX_CHARS: int = 600\n",
    ") -> list[str]:\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    node_children = node.children + [MockNode(node.end_byte, node.end_byte)]\n",
    "\n",
    "    for child, next_child in zip(node_children[:-1], node_children[1:]):\n",
    "        if child.end_byte - child.start_byte > MAX_CHARS:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = \"\"\n",
    "            chunks.extend(chunk_node(child, text, MAX_CHARS))\n",
    "        elif child.end_byte - child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = text[child.start_byte: next_child.start_byte]\n",
    "        else:\n",
    "            current_chunk += text[child.start_byte: next_child.start_byte]\n",
    "    chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "for chunk in chunk_node(tree.root_node, python_code):\n",
    "    print(chunk + \"\\n\\n====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, using Span's we can clean up the code a bit. Like removing the MockNode altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span(start=1, end=430)\n",
      "Span(start=432, end=696)\n",
      "Span(start=698, end=772)\n",
      "Span(start=777, end=1122)\n",
      "Span(start=1127, end=1502)\n",
      "Span(start=1502, end=1502)\n",
      "Span(start=1502, end=1502)\n",
      "Span(start=1505, end=2107)\n",
      "Span(start=2107, end=2107)\n",
      "Span(start=2110, end=2157)\n",
      "Span(start=2162, end=2759)\n",
      "Span(start=2764, end=3207)\n",
      "Span(start=3212, end=3442)\n",
      "Span(start=3447, end=3979)\n",
      "Span(start=3984, end=4426)\n",
      "Span(start=4426, end=4426)\n",
      "Span(start=4426, end=4426)\n"
     ]
    }
   ],
   "source": [
    "def chunk_node(\n",
    "    node: Node, \n",
    "    MAX_CHARS: int = 600,\n",
    ") -> list[Span]:\n",
    "    chunks: list[Span] = []\n",
    "    current_chunk: Span = Span(node.start_byte, node.start_byte)\n",
    "    node_children = node.children\n",
    "    for child in node_children:\n",
    "        if child.end_byte - child.start_byte > MAX_CHARS:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = Span(child.end_byte, child.end_byte)\n",
    "            chunks.extend(chunk_node(child, MAX_CHARS))\n",
    "        elif child.end_byte - child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = Span(child.start_byte, child.end_byte) \n",
    "        else:\n",
    "            current_chunk += Span(child.start_byte, child.end_byte)\n",
    "    chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "for chunk in chunk_node(tree.root_node):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping Whitespace when Measuring Length\n",
    "\n",
    "Gives heavily indented code the same number of lines per code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_len(s: str) -> int: # old len function\n",
    "\treturn len(s)\n",
    "\n",
    "def non_whitespace_len(s: str) -> int: # new len function\n",
    "    return len(re.sub(\"\\s\", \"\", s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coalescing Chunks\n",
    "\n",
    "Combining smaller chunks with larger ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import io\n",
      "import os\n",
      "import zipfile\n",
      "\n",
      "import openai\n",
      "import requests\n",
      "from loguru import logger\n",
      "\n",
      "from sweepai.core.gha_extraction import GHAExtractor\n",
      "from sweepai.events import CheckRunCompleted\n",
      "from sweepai.handlers.on_comment import on_comment\n",
      "from sweepai.utils.config.client import SweepConfig, get_gha_enabled\n",
      "from sweepai.utils.github_utils import get_github_client, get_token\n",
      "\n",
      "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
      "\n",
      "\n",
      "log_message = \"\"\"GitHub actions yielded the following error. \n",
      "\n",
      "{error_logs}\n",
      "\n",
      "This is likely a linting or type-checking issue with the source code but if you are updating the GitHub Actions or versioning, this could be an issue with the GitHub Action yaml files.\"\"\"\n",
      "\n",
      "\n",
      "def download_logs(repo_full_name: str, run_id: int, installation_id: int):\n",
      "\n",
      "    headers = {\n",
      "        \"Accept\": \"application/vnd.github+json\",\n",
      "        \"Authorization\": f\"Bearer {get_token(installation_id)}\",\n",
      "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
      "    }\n",
      "    response = requests.get(f\"https://api.github.com/repos/{repo_full_name}/actions/runs/{run_id}/logs\",\n",
      "                            headers=headers)\n",
      "\n",
      "    logs_str = \"\"\n",
      "\n",
      "    if response.status_code == 200:\n",
      "        zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
      "        for file in zip_file.namelist():\n",
      "            if \"/\" not in file:\n",
      "                with zip_file.open(file) as f:\n",
      "                    logs_str += f.read().decode(\"utf-8\")\n",
      "    else:\n",
      "        logger.warning(f\"Failed to download logs for run id: {run_id}\")\n",
      "    return logs_str\n",
      "\n",
      "\n",
      "\n",
      "def clean_logs(logs_str: str):\n",
      "    log_list = logs_str.split(\"\\n\")\n",
      "    truncated_logs = [log[log.find(\" \") + 1:] for log in log_list]\n",
      "    patterns = [\n",
      "        # for docker\n",
      "        \"Already exists\",\n",
      "        \"Pulling fs layer\",\n",
      "        \"Waiting\",\n",
      "        \"Download complete\",\n",
      "        \"Verifying Checksum\",\n",
      "        \"Pull complete\",\n",
      "        # For github\n",
      "        \"remote: Counting objects\",\n",
      "        \"remote: Compressing objects:\",\n",
      "        \"Receiving objects:\",\n",
      "        \"Resolving deltas:\"\n",
      "    ]\n",
      "    return \"\\n\".join([log.strip() for log in truncated_logs if not any(pattern in log for pattern in patterns)])\n",
      "\n",
      "\n",
      "\n",
      "def on_check_suite(request: CheckRunCompleted):\n",
      "    logger.info(f\"Received check run completed event for {request.repository.full_name}\")\n",
      "    g = get_github_client(request.installation.id)\n",
      "    repo = g.get_repo(request.repository.full_name)\n",
      "    if not get_gha_enabled(repo):\n",
      "        logger.info(f\"Skipping github action for {request.repository.full_name} because it is not enabled\")\n",
      "        return None\n",
      "    pr = repo.get_pull(request.check_run.pull_requests[0].number)\n",
      "    num_pr_commits = len(list(pr.get_commits()))\n",
      "    if num_pr_commits > 20:\n",
      "        logger.info(f\"Skipping github action for PR with {num_pr_commits} commits\")\n",
      "        return None\n",
      "\n",
      "    logger.info(f\"Running github action for PR with {num_pr_commits} commits\")\n",
      "    logs = download_logs(\n",
      "        request.repository.full_name,\n",
      "        request.check_run.run_id,\n",
      "        request.installation.id\n",
      "    )\n",
      "    if not logs:\n",
      "        return None\n",
      "    logs = clean_logs(logs)\n",
      "    extractor = GHAExtractor()\n",
      "    logger.info(f\"Extracting logs from {request.repository.full_name}, logs: {logs}\")\n",
      "    problematic_logs = extractor.gha_extract(logs)\n",
      "\n",
      "    if problematic_logs.count(\"\\n\") > 15:\n",
      "        problematic_logs += \"\\n\\nThere are a lot of errors. This is likely a larger issue with the PR and not a small linting/type-checking issue.\"\n",
      "    comments = list(pr.get_issue_comments())\n",
      "\n",
      "    if len(comments) >= 2 and problematic_logs == comments[-1].body and comments[-2].body == comments[-1].body:\n",
      "        comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs) + \"\\n\\nI'm getting the same errors 3 times in a row, so I will stop working on fixing this PR.\")\n",
      "        logger.warning(\"Skipping logs because it is duplicated\")\n",
      "        raise Exception(\"Duplicate error logs\")\n",
      "    print(problematic_logs)\n",
      "    comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs))\n",
      "\n",
      "    on_comment(\n",
      "        repo_full_name=request.repository.full_name,\n",
      "        repo_description=request.repository.description,\n",
      "        comment=problematic_logs,\n",
      "        pr_path=None,\n",
      "        pr_line_position=None,\n",
      "        username=request.sender.login,\n",
      "        installation_id=request.installation.id,\n",
      "        pr_number=request.check_run.pull_requests[0].number,\n",
      "        comment_id=comment.id,\n",
      "        repo=repo,\n",
      "    )\n",
      "    return {\"success\": True}\n"
     ]
    }
   ],
   "source": [
    "def coalesce_chunks(chunks: list[Span], source_code: str, coalesce: int = 50) -> list[Span]:\n",
    "    new_chunks = []\n",
    "    current_chunk = Span(0, 0)\n",
    "    for chunk in chunks:\n",
    "        current_chunk += chunk\n",
    "        if len(current_chunk) > coalesce and \"\\n\" in current_chunk.extract(source_code):\n",
    "            new_chunks.append(current_chunk)\n",
    "            current_chunk = Span(chunk.end, chunk.end)\n",
    "    if len(current_chunk) > 0:\n",
    "        new_chunks.append(current_chunk)\n",
    "    return new_chunks\n",
    "\n",
    "for chunk in coalesce_chunks(chunk_node(tree.root_node), python_code):\n",
    "    print(chunk.extract(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Line Numbers\n",
    "\n",
    "Using line numbers instead of character indices. Works because Span is unit-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: 0-15\n",
      "Chunk 1: 15-21\n",
      "Chunk 2: 21-23\n",
      "Chunk 3: 23-32\n",
      "Chunk 4: 32-41\n",
      "Chunk 5: 41-61\n",
      "Chunk 6: 61-75\n",
      "Chunk 7: 75-87\n",
      "Chunk 8: 87-90\n",
      "Chunk 9: 90-96\n",
      "Chunk 10: 96-109\n"
     ]
    }
   ],
   "source": [
    "def get_line_number(index: int, source_code: str) -> int:\n",
    "    total_chars = 0\n",
    "    for line_number, line in enumerate(source_code.splitlines(keepends=True), start=1):\n",
    "        total_chars += len(line)\n",
    "        if total_chars > index:\n",
    "            return line_number - 1\n",
    "    return line_number\n",
    "\n",
    "for i, chunk in enumerate(coalesce_chunks(chunk_node(tree.root_node), python_code)):\n",
    "    print(f\"Chunk {i}: {get_line_number(chunk.start, python_code)}-{get_line_number(chunk.end, python_code)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final New Algorithm\n",
    "\n",
    "Putting it altogether (switched back to MAX_CHARS=1500):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import io\n",
      "import os\n",
      "import zipfile\n",
      "\n",
      "import openai\n",
      "import requests\n",
      "from loguru import logger\n",
      "\n",
      "from sweepai.core.gha_extraction import GHAExtractor\n",
      "from sweepai.events import CheckRunCompleted\n",
      "from sweepai.handlers.on_comment import on_comment\n",
      "from sweepai.utils.config.client import SweepConfig, get_gha_enabled\n",
      "from sweepai.utils.github_utils import get_github_client, get_token\n",
      "\n",
      "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
      "\n",
      "log_message = \"\"\"GitHub actions yielded the following error. \n",
      "\n",
      "{error_logs}\n",
      "\n",
      "This is likely a linting or type-checking issue with the source code but if you are updating the GitHub Actions or versioning, this could be an issue with the GitHub Action yaml files.\"\"\"\n",
      "\n",
      "def download_logs(repo_full_name: str, run_id: int, installation_id: int):\n",
      "    headers = {\n",
      "        \"Accept\": \"application/vnd.github+json\",\n",
      "        \"Authorization\": f\"Bearer {get_token(installation_id)}\",\n",
      "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
      "    }\n",
      "    response = requests.get(f\"https://api.github.com/repos/{repo_full_name}/actions/runs/{run_id}/logs\",\n",
      "                            headers=headers)\n",
      "\n",
      "    logs_str = \"\"\n",
      "    if response.status_code == 200:\n",
      "        zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
      "        for file in zip_file.namelist():\n",
      "            if \"/\" not in file:\n",
      "                with zip_file.open(file) as f:\n",
      "                    logs_str += f.read().decode(\"utf-8\")\n",
      "    else:\n",
      "        logger.warning(f\"Failed to download logs for run id: {run_id}\")\n",
      "    return logs_str\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "def clean_logs(logs_str: str):\n",
      "    log_list = logs_str.split(\"\\n\")\n",
      "    truncated_logs = [log[log.find(\" \") + 1:] for log in log_list]\n",
      "    patterns = [\n",
      "        # for docker\n",
      "        \"Already exists\",\n",
      "        \"Pulling fs layer\",\n",
      "        \"Waiting\",\n",
      "        \"Download complete\",\n",
      "        \"Verifying Checksum\",\n",
      "        \"Pull complete\",\n",
      "        # For github\n",
      "        \"remote: Counting objects\",\n",
      "        \"remote: Compressing objects:\",\n",
      "        \"Receiving objects:\",\n",
      "        \"Resolving deltas:\"\n",
      "    ]\n",
      "    return \"\\n\".join([log.strip() for log in truncated_logs if not any(pattern in log for pattern in patterns)])\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "def on_check_suite(request: CheckRunCompleted):\n",
      "    logger.info(f\"Received check run completed event for {request.repository.full_name}\")\n",
      "    g = get_github_client(request.installation.id)\n",
      "    repo = g.get_repo(request.repository.full_name)\n",
      "    if not get_gha_enabled(repo):\n",
      "        logger.info(f\"Skipping github action for {request.repository.full_name} because it is not enabled\")\n",
      "        return None\n",
      "    pr = repo.get_pull(request.check_run.pull_requests[0].number)\n",
      "    num_pr_commits = len(list(pr.get_commits()))\n",
      "    if num_pr_commits > 20:\n",
      "        logger.info(f\"Skipping github action for PR with {num_pr_commits} commits\")\n",
      "        return None\n",
      "    logger.info(f\"Running github action for PR with {num_pr_commits} commits\")\n",
      "    logs = download_logs(\n",
      "        request.repository.full_name,\n",
      "        request.check_run.run_id,\n",
      "        request.installation.id\n",
      "    )\n",
      "    if not logs:\n",
      "        return None\n",
      "    logs = clean_logs(logs)\n",
      "    extractor = GHAExtractor()\n",
      "    logger.info(f\"Extracting logs from {request.repository.full_name}, logs: {logs}\")\n",
      "    problematic_logs = extractor.gha_extract(logs)\n",
      "    if problematic_logs.count(\"\\n\") > 15:\n",
      "        problematic_logs += \"\\n\\nThere are a lot of errors. This is likely a larger issue with the PR and not a small linting/type-checking issue.\"\n",
      "    comments = list(pr.get_issue_comments())\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "    if len(comments) >= 2 and problematic_logs == comments[-1].body and comments[-2].body == comments[-1].body:\n",
      "        comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs) + \"\\n\\nI'm getting the same errors 3 times in a row, so I will stop working on fixing this PR.\")\n",
      "        logger.warning(\"Skipping logs because it is duplicated\")\n",
      "        raise Exception(\"Duplicate error logs\")\n",
      "    print(problematic_logs)\n",
      "    comment = pr.as_issue().create_comment(log_message.format(error_logs=problematic_logs))\n",
      "    on_comment(\n",
      "        repo_full_name=request.repository.full_name,\n",
      "        repo_description=request.repository.description,\n",
      "        comment=problematic_logs,\n",
      "        pr_path=None,\n",
      "        pr_line_position=None,\n",
      "        username=request.sender.login,\n",
      "        installation_id=request.installation.id,\n",
      "        pr_number=request.check_run.pull_requests[0].number,\n",
      "        comment_id=comment.id,\n",
      "        repo=repo,\n",
      "    )\n",
      "\n",
      "====================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tree_sitter import Tree\n",
    "\n",
    "def chunker(\n",
    "\ttree: Tree,\n",
    "\tsource_code: bytes,\n",
    "\tMAX_CHARS=512 * 3,\n",
    "\tcoalesce=50 # Any chunk less than 50 characters long gets coalesced with the next chunk\n",
    ") -> list[Span]:\n",
    "\n",
    "    # 1. Recursively form chunks based on the last post (https://docs.sweep.dev/blogs/chunking-2m-files)\n",
    "    def chunk_node(node: Node) -> list[Span]:\n",
    "        chunks: list[Span] = []\n",
    "        current_chunk: Span = Span(node.start_byte, node.start_byte)\n",
    "        node_children = node.children\n",
    "        for child in node_children:\n",
    "            if child.end_byte - child.start_byte > MAX_CHARS:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = Span(child.end_byte, child.end_byte)\n",
    "                chunks.extend(chunk_node(child))\n",
    "            elif child.end_byte - child.start_byte + len(current_chunk) > MAX_CHARS:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = Span(child.start_byte, child.end_byte) \n",
    "            else:\n",
    "                current_chunk += Span(child.start_byte, child.end_byte)\n",
    "        chunks.append(current_chunk)\n",
    "        return chunks\n",
    "    chunks = chunk_node(tree.root_node)\n",
    "\n",
    "    # 2. Filling in the gaps\n",
    "    for prev, curr in zip(chunks[:-1], chunks[1:]):\n",
    "        prev.end = curr.start\n",
    "    curr.start = tree.root_node.end_byte\n",
    "\n",
    "    # 3. Combining small chunks with bigger ones\n",
    "    new_chunks = []\n",
    "    current_chunk = Span(0, 0)\n",
    "    for chunk in chunks:\n",
    "        current_chunk += chunk\n",
    "        if non_whitespace_len(current_chunk.extract(source_code)) > coalesce \\\n",
    "            and \"\\n\" in current_chunk.extract(source_code):\n",
    "            new_chunks.append(current_chunk)\n",
    "            current_chunk = Span(chunk.end, chunk.end)\n",
    "    if len(current_chunk) > 0:\n",
    "        new_chunks.append(current_chunk)\n",
    "\n",
    "    # 4. Changing line numbers\n",
    "    line_chunks = [Span(get_line_number(chunk.start, source_code),\n",
    "                    get_line_number(chunk.end, source_code)) for chunk in new_chunks]\n",
    "\n",
    "    # 5. Eliminating empty chunks\n",
    "    line_chunks = [chunk for chunk in line_chunks if len(chunk) > 0]\n",
    "\n",
    "    return line_chunks\n",
    "\n",
    "for chunk in chunker(tree, python_code):\n",
    "    print(chunk.extract_lines(python_code) + \"\\n\\n====================\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('sweepai-u_CIt3kb-py3.11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f22e56b0c638c2a35876232f2e2d6cfc31a0d98b7f3049980f1a4383610dba30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
